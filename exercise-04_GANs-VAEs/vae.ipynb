{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a376d2e",
   "metadata": {},
   "source": [
    "# Exercise 4: Generative Adverserial Networks and Auto Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d86b983",
   "metadata": {},
   "source": [
    "### Task:\n",
    "1. Build an autoencoder and train the model on MNIST dataset. Also visualize the reconstructed images and the latent space embedding.\n",
    "2. Add noise to the input data and adapt the autoencoder to a denoising autoencoder.\n",
    "3. Adapt the autoencoder to a variational autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712e31d",
   "metadata": {},
   "source": [
    "### Running notebooks on Google Colaboratory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39af2426",
   "metadata": {},
   "source": [
    "* Goto [Google Colab](https://colab.research.google.com/)  \n",
    "* Under `File` menu, click on `Upload notebook` and upload this notebook.  \n",
    "* To run the notebook with GPU, click on `Runtime` menu, select `Change runtime type`, and select `GPU` as Hardware accelerator.  \n",
    "  \n",
    "Note:  \n",
    "* If you see a `“RuntimeError: CUDA error: device-side assert triggered”` error, try running the notebook on CPU (by choosing `None` in Hardware accelerator as discussed above) to get a more detailed traceback for the error.  \n",
    "* If the notebook is running fine with CPU and the same error persists while using GPU, try restarting the runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UkygNdI5XcoK",
   "metadata": {
    "id": "UkygNdI5XcoK"
   },
   "source": [
    "## Normal Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f36feb",
   "metadata": {},
   "source": [
    "Install the needed packages in the current Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "id": "817d3fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T14:48:01.579928Z",
     "start_time": "2024-12-12T14:47:56.401608Z"
    }
   },
   "source": [
    "import sys\n",
    "#\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install torchvision\n",
    "!{sys.executable} -m pip install -U scikit-learn\n",
    "!{sys.executable} -m pip install tqdm"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.26.4)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.1.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.8.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.2.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (4.53.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.4.5)\r\n",
      "Requirement already satisfied: numpy>=1.21 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (24.1)\r\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (10.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (3.1.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.1.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.15.4)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (2024.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.1.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.17.2)\r\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: torch==2.2.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision) (2.2.2)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision) (10.4.0)\r\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.2->torchvision) (3.15.4)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.2->torchvision) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.2->torchvision) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.2->torchvision) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.2->torchvision) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch==2.2.2->torchvision) (2024.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch==2.2.2->torchvision) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch==2.2.2->torchvision) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.1.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.5.1)\r\n",
      "Collecting scikit-learn\r\n",
      "  Downloading scikit_learn-1.6.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\r\n",
      "Downloading scikit_learn-1.6.0-cp312-cp312-macosx_12_0_arm64.whl (11.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.2/11.2 MB\u001B[0m \u001B[31m23.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m0:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: scikit-learn\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.5.1\r\n",
      "    Uninstalling scikit-learn-1.5.1:\r\n",
      "      Successfully uninstalled scikit-learn-1.5.1\r\n",
      "Successfully installed scikit-learn-1.6.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.1.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.66.4)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.1.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "277c4ec5",
   "metadata": {},
   "source": [
    "PyTorch is an open source python package for building deep learning models. We will use this package in this exercise. For a basic understanding of the package, see [PyTorch introduction](https://pytorch.org/tutorials/beginner/basics/intro.html)."
   ]
  },
  {
   "cell_type": "code",
   "id": "77c5087e",
   "metadata": {
    "id": "77c5087e",
    "ExecuteTime": {
     "end_time": "2024-12-12T14:48:15.564272Z",
     "start_time": "2024-12-12T14:48:07.969368Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.manifold import TSNE\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "f1ca5cb3",
   "metadata": {},
   "source": [
    "### Set a random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "id": "d5b73efb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5b73efb",
    "outputId": "580d96cd-36c2-4240-eec2-daa93294f3a4",
    "ExecuteTime": {
     "end_time": "2024-12-12T14:48:17.522701Z",
     "start_time": "2024-12-12T14:48:17.517708Z"
    }
   },
   "source": [
    "seed = 12\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "print(\"Random Seed: \", seed)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  12\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "4213fc8a",
   "metadata": {},
   "source": [
    "Use GPU if available to accelerate training"
   ]
  },
  {
   "cell_type": "code",
   "id": "d94093fa",
   "metadata": {
    "id": "d94093fa",
    "ExecuteTime": {
     "end_time": "2024-12-12T14:48:20.250256Z",
     "start_time": "2024-12-12T14:48:20.225401Z"
    }
   },
   "source": "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "568fd9db",
   "metadata": {},
   "source": [
    "### Define the transformations needed on the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212bbf29",
   "metadata": {},
   "source": [
    "PyTorch has two primitives to work with data: `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`. `Dataset` stores the  data samples and their corresponding labels, and `DataLoader` wraps an iterable around the Dataset.  \n",
    "The `torchvision.datasets` module contains Dataset objects for many real-world vision data. We will use the MNIST dataset of handwritten digits in this exercise.  \n",
    "Every TorchVision Dataset includes two arguments: `transform` and `target_transform` to modify the data samples and labels respectively. You can use `torchvision.transforms.Compose` to compose multiple transformations together.  \n",
    "For our MNIST dataset, we can use the `torchvision.transforms.ToTensor` to convert the numpy array to a tensor and we will define a function to flatten the tensor to a 1D input using `torch.flatten`. `torchvision.transforms.Lambda` allows us to define any user defined transformations."
   ]
  },
  {
   "cell_type": "code",
   "id": "0b574e14",
   "metadata": {
    "id": "0b574e14",
    "ExecuteTime": {
     "end_time": "2024-12-12T14:48:23.954401Z",
     "start_time": "2024-12-12T14:48:23.952013Z"
    }
   },
   "source": [
    "# Creating the transformations\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(torch.flatten),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "7d4648e8",
   "metadata": {},
   "source": [
    "Download the MNIST dataset and define the training and testing sets by applying the previously defined transformations."
   ]
  },
  {
   "cell_type": "code",
   "id": "0dc4a0f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845,
     "referenced_widgets": [
      "06b17cdf7b784a85b22db7c70d6e01d3",
      "14cb4590c59f4c8389c0a3590d146334",
      "958896b05c084fda82c421d25554400e",
      "5e72b322de094e568643e6df426bb74d",
      "e30c2b72bcd948d4a62e9d3291bd8a9f",
      "46e6cde6ef694b159baced9cebfd9d6b",
      "3d6f2c2df8f24914aa5753c81eb96a6d",
      "e1b2c4ea87c147c79242653e97025d25",
      "ef53cdaf9a6141b2926a4777fd6d4df6",
      "4528d93bf18d484c81d34f05d5c48b12",
      "dcf6a246ad8c459d90999c7824086100",
      "1037d7c4aa19480bab0d9a1c62b60098",
      "21265d659bdc450e8ed0666abfd3e2e1",
      "64fe43e5e3ef4e989f08cdb12c9d5deb",
      "1e68decb543e47179202467ea56f6297",
      "c843c5fdd03c4a9686eb50b964319165",
      "98b8af81d4c0403c9644b06cd4a8c8f2",
      "566620b3fcdb4fa180270df3b52082ba",
      "9d7bc4d9344b4a70acb54673a02a310d",
      "c498097f22634dc9ae5a3ac99f8d130c",
      "cd19182aae1d41729c2cd33fb3d85758",
      "ae23ac47c2754558adbcb8af521ab851",
      "52897395276b41df9c9acf7a8829c408",
      "888d5be9538c461a9cb1759a421886b2",
      "31fe1fdcdc4a4f45a5f210bc7534b203",
      "df7347a664b44f03a3c2db844d7cffee",
      "7f84912b43e1436e8b22abd8473d681e",
      "3812ba0ecb8e4bad976cf113920b06f8",
      "94f2effe858446f4a80e870a940f6eaa",
      "c87e2778f4e148d1a96dc829d2104f95",
      "6e004aa9cf544be2ade9cf85690ba068",
      "95560c4decf743fa891d69b8a38931f8"
     ]
    },
    "id": "0dc4a0f4",
    "outputId": "31f6af29-1985-44f9-b60a-5b62979c97cc",
    "ExecuteTime": {
     "end_time": "2024-12-12T15:16:21.156244Z",
     "start_time": "2024-12-12T15:16:20.456089Z"
    }
   },
   "source": [
    "# Define the train and test sets from MNIST data\n",
    "import urllib.request\n",
    "\n",
    "dset_train = MNIST(\"./data\", train=True, transform=transform, download=True)\n",
    "dset_test = MNIST(\"./data\", train=False, transform=transform, download=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error downloading train-images-idx3-ubyte.gz",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Define the train and test sets from MNIST data\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01murllib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrequest\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m dset_train \u001B[38;5;241m=\u001B[39m \u001B[43mMNIST\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./data\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m dset_test \u001B[38;5;241m=\u001B[39m MNIST(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./data\u001B[39m\u001B[38;5;124m\"\u001B[39m, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, transform\u001B[38;5;241m=\u001B[39mtransform, download\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/mnist.py:99\u001B[0m, in \u001B[0;36mMNIST.__init__\u001B[0;34m(self, root, train, transform, target_transform, download)\u001B[0m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m download:\n\u001B[0;32m---> 99\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_exists():\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset not found. You can use download=True to download it\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/mnist.py:195\u001B[0m, in \u001B[0;36mMNIST.download\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    193\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 195\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError downloading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilename\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error downloading train-images-idx3-ubyte.gz"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "818cbd66",
   "metadata": {},
   "source": [
    "### Define the dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2937b8c5",
   "metadata": {},
   "source": [
    "Here we define a batch size for the dataloader, i.e. each element in the dataloader iterable will return a batch of features and labels. Additionally, we can shuffle the data in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "id": "281a4da7",
   "metadata": {
    "id": "281a4da7",
    "ExecuteTime": {
     "end_time": "2024-12-12T14:49:29.488913Z",
     "start_time": "2024-12-12T14:49:29.479372Z"
    }
   },
   "source": [
    "batch_size = 64\n",
    "\n",
    "# We use DataLoader to get the images of the training set batch by batch.\n",
    "train_dataloader = DataLoader(dset_train, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dset_test, batch_size=batch_size, shuffle=True)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dset_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m64\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# We use DataLoader to get the images of the training set batch by batch.\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m train_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(\u001B[43mdset_train\u001B[49m, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m test_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(dset_test, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'dset_train' is not defined"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "958053e6",
   "metadata": {},
   "source": [
    "### Plotting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ba60bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "65ba60bf",
    "outputId": "72f6c7c8-b0cc-40e3-de0d-3c5e6317d7a8"
   },
   "outputs": [],
   "source": [
    "# Plot a few MNIST examples\n",
    "\n",
    "# Load a batch of images into memory\n",
    "images, labels = next(iter(train_dataloader))\n",
    "images = make_grid(\n",
    "    images.view(batch_size, 1, 28, 28),\n",
    "    padding=2,\n",
    "    normalize=True,\n",
    ").cpu()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"MNIST: Training Images\")\n",
    "plt.imshow(np.transpose(images, (1, 2, 0)))\n",
    "plt.show()\n",
    "\n",
    "print(\"Labels:\")\n",
    "print(labels.reshape(-1, 8).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f6c2a",
   "metadata": {},
   "source": [
    "### Construct a class for autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561bd030",
   "metadata": {},
   "source": [
    "Define an autoencoder class with the following dimensions for the hidden layers:  \n",
    "**Encoder:** $\\left[256, 128\\right]$  \n",
    "**Latent dimension:** To be specified by user at the time of initialization  \n",
    "**Decoder:** $\\left[128, 256\\right]$ \n",
    "\n",
    "Please refer [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html), and [activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) for defining the encoder and decoder. \n",
    "The class is to be inherited from the base [Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c53174a",
   "metadata": {
    "id": "5c53174a",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-956378cec57def2e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define size of input image features\n",
    "# For MNIST data, we flatten the 28x28 image\n",
    "num_input_features = 28**2\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, num_latent_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_latent_features = num_latent_features\n",
    "        # Define encoder network to encode the data onto the latent space using two linear layers.\n",
    "        # self.encoder = ...\n",
    "        \n",
    "        # Encode the data onto the latent space using two linear layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Input layer\n",
    "            nn.Linear(in_features=num_input_features, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            # 1st hidden layer\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            # Latent layer\n",
    "            nn.Linear(in_features=128, out_features=self.num_latent_features),\n",
    "        )\n",
    "\n",
    "        # Define decoder network to decode the latent information into the original image.\n",
    "        # self.decoder = ...\n",
    "        \n",
    "        # To decode latent information into the original image\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 1st Hidden layer\n",
    "            nn.Linear(in_features=self.num_latent_features, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            # Next hidden layer\n",
    "            nn.Linear(in_features=128, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            # Output layer\n",
    "            nn.Linear(in_features=256, out_features=num_input_features),\n",
    "            # The inputs are greyscale images with values on the scale [0, 1]\n",
    "            # Convert the output to the scale\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = {}\n",
    "\n",
    "        # Pass the input through encoder to get the latent representation\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        # Pass the latent information through decoder to reconstruct the input\n",
    "        x = self.decoder(z)\n",
    "\n",
    "        # Return both the latent representation and output reconstruction\n",
    "        outputs[\"x_out\"] = x\n",
    "        outputs[\"z\"] = z\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deab6fd",
   "metadata": {},
   "source": [
    "Initialize the model with a latent dimension of 10.   \n",
    "You can also print the model to see the general structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0e56c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88a0e56c",
    "outputId": "e65153bb-fabd-4f6e-8b57-7fb249294f1e"
   },
   "outputs": [],
   "source": [
    "num_latent_features = 10\n",
    "model = AutoEncoder(num_latent_features).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218f15bd",
   "metadata": {},
   "source": [
    "### Define the loss function and optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e912c227",
   "metadata": {},
   "source": [
    "Define a loss function including L1 regularization on latent representation. The weightage of regularization in the total loss is to be controlled.  \n",
    "Also define an optimizer for updating the parameter weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f5dfb",
   "metadata": {
    "id": "b61f5dfb",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f2429c7547788d6f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ae_loss(x_recons, x):\n",
    "    \"\"\"\n",
    "    Loss function for the autoencoder with an additional l1 regularization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_recons : array_like\n",
    "        Decoder output.\n",
    "    x : array_like\n",
    "        Input to the encoder.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        resulting autoencoder loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    return mse_loss(x_recons, x)\n",
    "    \n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = ae_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d498174",
   "metadata": {},
   "source": [
    "### Train the auto encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af3f182",
   "metadata": {},
   "source": [
    "Each module inherited from `nn.Module` has a `self.training` attribute which should be set to `True` while training and `False` while testing. You can use the `train()` function to set the model in taining mode and `eval()` function to set the model in testing mode. This is needed because some layers (like batch normalization, droput, etc.) behaves differently while training and testing.  \n",
    "\n",
    "**Perform the following steps while training the model:**  \n",
    "* Iterate over batches of input data in each epoch.\n",
    "* PyTorch accumulates the parameter gradients on each subsequent backward pass, thus we need to set the gradients explicitly to zero in each iteration to avoid this. This can be done by calling function `zero_grad()` on the optimizer.\n",
    "* Perform a forward pass in the model with the input batch.\n",
    "* Calculate the loss for this batch of input.\n",
    "* Calculate the gradients for the model parameters in the direction of minimizing the loss. The `backward()` function is used on the loss to calculate this gradients.\n",
    "* Perform the optimization of model parameters using the gradients. The `step()` function is used on the optimizer to update the parameters.\n",
    "\n",
    "The batch loss and/or epoch loss can be printed out to visualize the training progress.  \n",
    "\n",
    "**Perform the following while testing the model:** \n",
    "* Set the model to testing mode.\n",
    "* Disable the gradient calculation. Take a look at [`torch.no_grad()`](https://pytorch.org/docs/stable/generated/torch.no_grad.html).\n",
    "* Calculate the test set loss.\n",
    "\n",
    "When using a tensor for operations that does not require gradient calculations (for debugging, plotting, etc.), you need to break it from the computational graph. This can be done by calling the `detach()` function on the tensor. A tensor on GPU can be moved to CPU by calling the `cpu()` function on the tensor.\n",
    "\n",
    "⚠️ The training can take a long time, from minutes to half an hour or even more, depending on your hardware!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb936a0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ce2281e3d69b82da",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    ### =====TRAINING=====\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as train_epoch_pbar:\n",
    "        for data, labels in train_epoch_pbar:\n",
    "            train_epoch_pbar.set_description(f\"Epoch {epoch}\")\n",
    "            data = data.to(device)\n",
    "\n",
    "            \"\"\"\n",
    "            Calculate the loss for the current batch of inputs and \n",
    "            optimize the network parameters using the gradients.\n",
    "            Variable `loss` should hold the loss value for current batch which \n",
    "            will be used to display the progress of training.\n",
    "            \"\"\"\n",
    "            # loss = ...\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = loss_function(outputs[\"x_out\"], data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            train_loss += loss.item()\n",
    "            avg_batch_loss = loss.item() / len(data)\n",
    "            train_epoch_pbar.set_postfix(loss=avg_batch_loss)\n",
    "\n",
    "        avg_epoch_loss = train_loss / len(train_dataloader.dataset)\n",
    "        print(\n",
    "            f\"======= Epoch: {epoch} Average Training loss: {avg_epoch_loss:.4f} =======\"\n",
    "        )\n",
    "        train_loss_list.append(avg_epoch_loss)\n",
    "\n",
    "    ### =====TESTING=====\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as test_pbar:\n",
    "            for data, labels in test_pbar:\n",
    "                data = data.to(device)\n",
    "                \"\"\"\n",
    "                Calculate the loss for the current test batch of inputs.\n",
    "                Variable `loss` should hold the test set loss value for current batch which \n",
    "                will be used to display the progress of training.\n",
    "                \"\"\"\n",
    "                # test_loss = ...\n",
    "                \n",
    "                outputs = model(data)\n",
    "                loss = loss_function(outputs[\"x_out\"], data)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    print(f\"======= Test set loss: {test_loss:.4f} =======\")\n",
    "    test_loss_list.append(test_loss)\n",
    "\n",
    "    # Visualizing training progress\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(batch_size, num_latent_features).to(device)\n",
    "        sample = model.decoder(sample).detach().cpu()\n",
    "        image_grid = make_grid(sample.view(batch_size, 1, 28, 28), nrow=8).permute(\n",
    "            1, 2, 0\n",
    "        )\n",
    "        plt.imshow(image_grid.numpy(), cmap=\"gray\", vmin=0, vmax=255)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Epoch {epoch}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85a887",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "55aa53ff",
    "outputId": "a669e988-f7c0-4d8b-cca4-7890287d0bef"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "f, axarr = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "# Loss\n",
    "ax = axarr\n",
    "ax.set_title(\"Loss\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "\n",
    "ax.plot(np.arange(epochs), train_loss_list)\n",
    "ax.plot(np.arange(epochs), test_loss_list, linestyle=\"--\")\n",
    "ax.legend([\"Training\", \"Testing\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a93c71",
   "metadata": {},
   "source": [
    "### Visualizing input data and its reconstruction from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XCSkQbz8ZEBq",
   "metadata": {
    "id": "XCSkQbz8ZEBq"
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 2, figsize=(12, 12))\n",
    "\n",
    "with torch.no_grad():\n",
    "    x, _ = next(iter(train_dataloader))\n",
    "    x = Variable(x)\n",
    "    x = x.to(device)\n",
    "    outputs = model(x)\n",
    "    x_out = outputs[\"x_out\"].detach().cpu()\n",
    "    x = x.detach().cpu()\n",
    "\n",
    "    image_grid = make_grid(x.view(batch_size, 1, 28, 28), nrow=8).permute(1, 2, 0)\n",
    "    axarr[0].imshow(image_grid.numpy())\n",
    "    axarr[0].set_title(\"Input\")\n",
    "    axarr[0].axis(\"off\")\n",
    "\n",
    "    image_grid = make_grid(x_out.view(batch_size, 1, 28, 28), nrow=8).permute(1, 2, 0)\n",
    "    axarr[1].imshow(image_grid.numpy())\n",
    "    axarr[1].set_title(\"Reconstructed input\")\n",
    "    axarr[1].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a183319c",
   "metadata": {},
   "source": [
    "### Visualize the latent space embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ecf80",
   "metadata": {},
   "source": [
    "The latent space embedding can be visualized by projecting into 2D space using [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tQlc27pxvL1q",
   "metadata": {
    "id": "tQlc27pxvL1q"
   },
   "outputs": [],
   "source": [
    "def generate_visualisation(latent, labels):\n",
    "    latent_embedded = TSNE(\n",
    "        n_components=2, init=\"random\", learning_rate=\"auto\"\n",
    "    ).fit_transform(latent)\n",
    "    N = 10\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(\n",
    "        latent_embedded[:, 0],\n",
    "        latent_embedded[:, 1],\n",
    "        c=labels,\n",
    "        marker=\"o\",\n",
    "        edgecolor=\"none\",\n",
    "        cmap=\"Spectral\",\n",
    "    )\n",
    "    plt.colorbar(boundaries=np.arange(N + 1) - 0.5).set_ticks(np.arange(N))\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "def visualize_embeddings(model, train_dataloader):\n",
    "    with torch.no_grad():\n",
    "        x, labels = next(iter(train_dataloader))\n",
    "        x = Variable(x)\n",
    "        x = x.to(device)\n",
    "        outputs = model(x)\n",
    "        z = outputs[\"z\"].cpu()\n",
    "        generate_visualisation(z, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44135310",
   "metadata": {},
   "source": [
    "To generate a nice plot, we need sufficient amount of latent data points. This can be done by temporarily increasing the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7hzkdCjJbbEn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "7hzkdCjJbbEn",
    "outputId": "9e7813c6-7bed-44a4-8d3a-2a23c4aca58c"
   },
   "outputs": [],
   "source": [
    "# Increase the batch size for a nicer plot\n",
    "dataloader = DataLoader(dset_train, batch_size=512, shuffle=True)\n",
    "visualize_embeddings(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daqd14w3vhWw",
   "metadata": {
    "id": "daqd14w3vhWw"
   },
   "source": [
    "# Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9161ac34",
   "metadata": {},
   "source": [
    "Denoising autoencoders prevents the model from *learning* the data. The input data is corrupted by noise before passing through the model.   \n",
    "We define a module for the addition of noise during training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Nrp9ayyWfwBF",
   "metadata": {
    "id": "Nrp9ayyWfwBF"
   },
   "outputs": [],
   "source": [
    "normal = torch.distributions.Normal(0.5, 0.5)\n",
    "\n",
    "\n",
    "class AdditiveGaussianNoise(nn.Module):\n",
    "    def __init__(self, plot=False):\n",
    "        super().__init__()\n",
    "        self.plot = plot\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Every PyTorch Module object has a self.training boolean which can be used\n",
    "        to check if we are in training (True) or evaluation (False) mode.\"\"\"\n",
    "        if self.training or self.plot:\n",
    "            device = x.device\n",
    "            return x + normal.sample(sample_shape=torch.Size(x.shape)).to(device)\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35c3ab9",
   "metadata": {},
   "source": [
    "### Construct a class for denoising autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63dde75",
   "metadata": {},
   "source": [
    "Change the `AutoEncoder` class previously defined to include the `AdditiveGaussianNoise` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N24FrI1qgxKN",
   "metadata": {
    "id": "N24FrI1qgxKN",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e7f90980405dd5a2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "num_input_features = 28**2\n",
    "\n",
    "\n",
    "class DenoisingAutoEncoder(AutoEncoder):\n",
    "    def __init__(self, num_latent_features):\n",
    "        super().__init__(num_latent_features)\n",
    "\n",
    "        # Modify the encoder network of `AutoEncoder` class to use the noise injection to input.\n",
    "        # self.encoder = ...\n",
    "        \n",
    "        # Encode the data onto the latent space using two linear layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Added noise to the input data\n",
    "            AdditiveGaussianNoise(),\n",
    "            # Input layer\n",
    "            nn.Linear(in_features=num_input_features, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            # 1st hidden layer\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            # Latent layer characterised by its mean and variance\n",
    "            nn.Linear(in_features=128, out_features=self.num_latent_features),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IlK7NCNSvp4K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlK7NCNSvp4K",
    "outputId": "56838434-faae-4a49-b5fe-5f84c8e30722"
   },
   "outputs": [],
   "source": [
    "num_latent_features = 10\n",
    "model = DenoisingAutoEncoder(num_latent_features).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c0864e",
   "metadata": {},
   "source": [
    "### Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QQ7mjyk3z8B4",
   "metadata": {
    "id": "QQ7mjyk3z8B4"
   },
   "outputs": [],
   "source": [
    "\"\"\"Define the optimizer parameters\"\"\"\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = ae_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec863f7",
   "metadata": {},
   "source": [
    "### Plot the noisy input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vl7q7js7w9S0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "vl7q7js7w9S0",
    "outputId": "571c2f8e-d824-4b1e-a0ac-6c08b07cf9b2"
   },
   "outputs": [],
   "source": [
    "# plot the noisy examples\n",
    "f, axarr = plt.subplots(4, 16, figsize=(16, 4))\n",
    "\n",
    "# Load a batch of images into memory\n",
    "images, labels = next(iter(train_dataloader))\n",
    "noise = AdditiveGaussianNoise(plot=True)\n",
    "for i, ax in enumerate(axarr.flat):\n",
    "    ax.imshow(noise(images[i]).view(28, 28), cmap=\"binary_r\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"MNIST digits with added noise\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd1ccca",
   "metadata": {},
   "source": [
    "### Train the denoising autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537742e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e284f296d9200bff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    ### =====TRAINING=====\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as train_epoch_pbar:\n",
    "        for data, labels in train_epoch_pbar:\n",
    "            train_epoch_pbar.set_description(f\"Epoch {epoch}\")\n",
    "            data = data.to(device)\n",
    "\n",
    "            \"\"\"\n",
    "            Calculate the loss for the current batch of inputs and \n",
    "            optimize the network parameters using the gradients.\n",
    "            Variable `loss` should hold the loss value for current batch which \n",
    "            will be used to display the progress of training.\n",
    "            \"\"\"\n",
    "            # loss = ...\n",
    "           \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = loss_function(outputs[\"x_out\"], data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            train_loss += loss.item()\n",
    "            avg_batch_loss = loss.item() / len(data)\n",
    "            train_epoch_pbar.set_postfix(loss=avg_batch_loss)\n",
    "\n",
    "        avg_epoch_loss = train_loss / len(train_dataloader.dataset)\n",
    "        print(\n",
    "            f\"======= Epoch: {epoch} Average Training loss: {avg_epoch_loss:.4f} =======\"\n",
    "        )\n",
    "        train_loss_list.append(avg_epoch_loss)\n",
    "\n",
    "    ### =====TESTING=====\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as test_pbar:\n",
    "            for data, labels in test_pbar:\n",
    "                data = data.to(device)\n",
    "                \"\"\"\n",
    "                Calculate the loss for the current test batch of inputs.\n",
    "                Variable `loss` should hold the test set loss value for current batch which \n",
    "                will be used to display the progress of training.\n",
    "                \"\"\"\n",
    "                # test_loss = ...\n",
    "                \n",
    "                outputs = model(data)\n",
    "                loss = loss_function(outputs[\"x_out\"], data)\n",
    "                test_loss += loss.item()\n",
    "                \n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    print(f\"======= Test set loss: {test_loss:.4f} =======\")\n",
    "    test_loss_list.append(test_loss)\n",
    "\n",
    "    # Visualizing training progress\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(batch_size, num_latent_features).to(device)\n",
    "        sample = model.decoder(sample).detach().cpu()\n",
    "        image_grid = make_grid(sample.view(batch_size, 1, 28, 28), nrow=8).permute(\n",
    "            1, 2, 0\n",
    "        )\n",
    "        plt.imshow(image_grid.numpy(), cmap=\"gray\", vmin=0, vmax=255)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Epoch {epoch}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c62503",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "55aa53ff",
    "outputId": "a669e988-f7c0-4d8b-cca4-7890287d0bef"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "f, axarr = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "# Loss\n",
    "ax = axarr\n",
    "ax.set_title(\"Loss\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "\n",
    "ax.plot(np.arange(epochs), train_loss_list)\n",
    "ax.plot(np.arange(epochs), test_loss_list, linestyle=\"--\")\n",
    "ax.legend([\"Training\", \"Testing\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6078b",
   "metadata": {},
   "source": [
    "### Visualizing input data and its reconstruction from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f46dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 2, figsize=(12, 12))\n",
    "\n",
    "with torch.no_grad():\n",
    "    x, _ = next(iter(train_dataloader))\n",
    "    x = Variable(x)\n",
    "    x = x.to(device)\n",
    "    outputs = model(x)\n",
    "    x_out = outputs[\"x_out\"].detach().cpu()\n",
    "    x = x.detach().cpu()\n",
    "\n",
    "    image_grid = make_grid(x.view(batch_size, 1, 28, 28), nrow=8).permute(1, 2, 0)\n",
    "    axarr[0].imshow(image_grid.numpy())\n",
    "    axarr[0].set_title(\"Input\")\n",
    "    axarr[0].axis(\"off\")\n",
    "\n",
    "    image_grid = make_grid(x_out.view(batch_size, 1, 28, 28), nrow=8).permute(1, 2, 0)\n",
    "    axarr[1].imshow(image_grid.numpy())\n",
    "    axarr[1].set_title(\"Reconstructed input\")\n",
    "    axarr[1].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ee5cc9",
   "metadata": {},
   "source": [
    "### Visualize the latent space embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5s2lbXsrzVCo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "5s2lbXsrzVCo",
    "outputId": "837ff46e-1a3a-4e8a-fe95-c9dc41f47337"
   },
   "outputs": [],
   "source": [
    "# Increase the batch size for a nicer plot\n",
    "dataloader = DataLoader(dset_train, batch_size=512, shuffle=True)\n",
    "visualize_embeddings(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4rkTiC_uXzGb",
   "metadata": {
    "id": "4rkTiC_uXzGb"
   },
   "source": [
    "# Variational AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a931e7d",
   "metadata": {},
   "source": [
    "Define a Variational Autoencoder by formulating the encoder to describe a probability distribution for each latent attribute. The latent dimension is characterized by mean and variance for each latent attribute.  \n",
    "\n",
    "During the forward pass, you can use the `torch.chunk` function to split the latent layer to mean and variance vectors.  \n",
    "\n",
    "Use the reparametrization trick suggested in the [VAE paper](https://arxiv.org/pdf/1312.6114.pdf) for sampling the latent dimension for input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f8a10e",
   "metadata": {
    "id": "98f8a10e",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c5b5b1b2a06ec6f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define size of image features\n",
    "# For MNIST data, we flatten the 28x28 image\n",
    "num_input_features = 28**2\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(AutoEncoder):\n",
    "    def __init__(self, num_latent_features):\n",
    "        super().__init__(num_latent_features)\n",
    "        \"\"\"\n",
    "        Modify the encoder network of `AutoEncoder` class to have \n",
    "        both mean and variance for each latent attribute.\n",
    "        \"\"\"\n",
    "        # self.encoder = ...\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            # Input layer\n",
    "            nn.Linear(in_features=num_input_features, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            # 1st hidden layer\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            # Latent layer characterised by its mean and variance\n",
    "            nn.Linear(in_features=128, out_features=2 * self.num_latent_features),\n",
    "        )\n",
    "\n",
    "\n",
    "    def reparametrisation(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        Generate a sample from the latent distribution.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mu : array_like\n",
    "             mean of the latent distribution.\n",
    "        log_var : array_like\n",
    "                  log of variance of the latent distribution.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sample : array_like\n",
    "                 A sample latent representation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # This is needed because you can not backprob through a random node\n",
    "        # N(mu, sigma) = mu + epsilon * sigma, where epsilon ~ N(0, 1)\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        sample = mu + eps * std\n",
    "        return sample\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = {}\n",
    "\n",
    "        # Split encoder outputs into a mean and variance vector\n",
    "        mu, log_var = torch.chunk(self.encoder(x), 2, dim=-1)\n",
    "\n",
    "        z = self.reparametrisation(mu, log_var)\n",
    "\n",
    "        # Run through decoder\n",
    "        x = self.decoder(z)\n",
    "\n",
    "        outputs[\"x_out\"] = x\n",
    "        outputs[\"z\"] = z\n",
    "        outputs[\"mu\"] = mu\n",
    "        outputs[\"log_var\"] = log_var\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73ef64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc73ef64",
    "outputId": "c2fba4e3-7f77-4e94-d0a2-969a1106832b"
   },
   "outputs": [],
   "source": [
    "num_latent_features = 10\n",
    "model = VariationalAutoEncoder(num_latent_features).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce728455",
   "metadata": {},
   "source": [
    "### Construct the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1597b7",
   "metadata": {
    "id": "1f1597b7"
   },
   "source": [
    "To ensure that the approximate learned distribution $q(z|x)$ is similar to $p(z|x)$, we minimize the KL divergence between the two distributions.  \n",
    "\n",
    "$\\min K L(q(z \\mid x) \\| p(z \\mid x))$  \n",
    "\n",
    "We can minimize the above expression by maximizing the following:  \n",
    "$\\mathcal{L}(x) = \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z)\\right] - \\mathbb{E}_{q(z|x)}\\left[\\log \\frac{q(z|x)}{p(z)}\\right] = \\mathbb{E}_{q(z|x)} \\left[\\log p(x|z)\\right] - KL[q(z|x) || p(z)]$\n",
    "\n",
    "The first term represents the reconstruction likelihood and the second term ensures that our learned distribution $q$ is similar to the true prior distribution $p$. The KL-term can be calculated analytically and for the reconstruction error, $\\log p(x|z)$, we can use the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a842eb73",
   "metadata": {
    "id": "a842eb73",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-593cfcef164d820e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def ELBO_loss(x_out, x, mu, log_var):\n",
    "    \"\"\"\n",
    "    Calculate the ELBO loss for VAE.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_out : array_like\n",
    "            Output of the decoder network.\n",
    "    x : array_like\n",
    "        Input to the decoder network.\n",
    "    mu : array_like\n",
    "         Mean of the latent distribution.\n",
    "    log_var : array_like\n",
    "              Log of variance of the latent distribution.\n",
    "    Returns\n",
    "    -------\n",
    "    loss : array_like\n",
    "           Calculated ELBO loss.\n",
    "    \"\"\"\n",
    "    MSE = mse_loss(x_out, x, reduction=\"sum\")\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    loss = MSE + KLD\n",
    "    return loss\n",
    "\n",
    "\n",
    "\"\"\"Define the optimizer parameters\"\"\"\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = ELBO_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cfb4cf",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ed5fb",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86ff230cf087e813",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    ### =====TRAINING=====\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as train_epoch_pbar:\n",
    "        for data, labels in train_epoch_pbar:\n",
    "            train_epoch_pbar.set_description(f\"Epoch {epoch}\")\n",
    "            data = data.to(device)\n",
    "\n",
    "            \"\"\"\n",
    "            Calculate the loss for the current batch of inputs and \n",
    "            optimize the network parameters using the gradients.\n",
    "            Variable `loss` should hold the loss value for current batch which \n",
    "            will be used to display the progress of training.\n",
    "            \"\"\"\n",
    "            # loss = ...\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = loss_function(\n",
    "                outputs[\"x_out\"], data, outputs[\"mu\"], outputs[\"log_var\"]\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            avg_batch_loss = loss.item() / len(data)\n",
    "            train_epoch_pbar.set_postfix(loss=avg_batch_loss)\n",
    "\n",
    "        avg_epoch_loss = train_loss / len(train_dataloader.dataset)\n",
    "        print(\n",
    "            f\"======= Epoch: {epoch} Average Training loss: {avg_epoch_loss:.4f} =======\"\n",
    "        )\n",
    "\n",
    "    ### =====TESTING=====\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as test_pbar:\n",
    "            for data, labels in test_pbar:\n",
    "                data = data.to(device)\n",
    "                \"\"\"\n",
    "                Calculate the loss for the current test batch of inputs.\n",
    "                Variable `loss` should hold the test set loss value for current batch which \n",
    "                will be used to display the progress of training.\n",
    "                \"\"\"\n",
    "                # test_loss = ...\n",
    "                \n",
    "                outputs = model(data)\n",
    "                loss = loss_function(\n",
    "                    outputs[\"x_out\"], data, outputs[\"mu\"], outputs[\"log_var\"]\n",
    "                )\n",
    "                test_loss += loss.item()\n",
    "                \n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    print(f\"======= Test set loss: {test_loss:.4f} =======\")\n",
    "\n",
    "    # Visualizing training progress\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(batch_size, num_latent_features).to(device)\n",
    "        sample = model.decoder(sample).detach().cpu()\n",
    "        image_grid = make_grid(sample.view(batch_size, 1, 28, 28), nrow=8).permute(\n",
    "            1, 2, 0\n",
    "        )\n",
    "        plt.imshow(image_grid.numpy(), cmap=\"gray\", vmin=0, vmax=255)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Epoch {epoch}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4ef3b",
   "metadata": {},
   "source": [
    "### Plot the training loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa53ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "55aa53ff",
    "outputId": "a669e988-f7c0-4d8b-cca4-7890287d0bef"
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "f, axarr = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "# Loss\n",
    "ax = axarr\n",
    "ax.set_title(\"ELBO Loss\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Error\")\n",
    "\n",
    "ax.plot(np.arange(epochs), train_loss_list)\n",
    "ax.plot(np.arange(epochs), test_loss_list, linestyle=\"--\")\n",
    "ax.legend([\"Training\", \"Testing\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7261abf",
   "metadata": {},
   "source": [
    "### Visualizing input data and its reconstruction from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35592e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 2, figsize=(12, 12))\n",
    "\n",
    "with torch.no_grad():\n",
    "    x, _ = next(iter(train_dataloader))\n",
    "    x = Variable(x)\n",
    "    x = x.to(device)\n",
    "    outputs = model(x)\n",
    "    x_out = outputs[\"x_out\"].detach().cpu()\n",
    "    x = x.detach().cpu()\n",
    "\n",
    "    image_grid = make_grid(x.view(batch_size, 1, 28, 28), nrow=8).permute(1, 2, 0)\n",
    "    axarr[0].imshow(image_grid.numpy())\n",
    "    axarr[0].set_title(\"Input\")\n",
    "    axarr[0].axis(\"off\")\n",
    "\n",
    "    image_grid = make_grid(x_out.view(batch_size, 1, 28, 28), nrow=8).permute(1, 2, 0)\n",
    "    axarr[1].imshow(image_grid.numpy())\n",
    "    axarr[1].set_title(\"Reconstructed input\")\n",
    "    axarr[1].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b678f36",
   "metadata": {},
   "source": [
    "### Visualize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t3lENedL_1Ki",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "t3lENedL_1Ki",
    "outputId": "787ef441-7097-4739-c622-b267825382b8"
   },
   "outputs": [],
   "source": [
    "# Increase the batch size for a nicer plot\n",
    "dataloader = DataLoader(dset_train, batch_size=512, shuffle=True)\n",
    "visualize_embeddings(model, dataloader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Create Assignment",
  "colab": {
   "collapsed_sections": [],
   "name": "vae.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
