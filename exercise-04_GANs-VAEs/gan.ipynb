{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5814aee9",
   "metadata": {},
   "source": [
    "# Exercise 4: Generative Adverserial Networks and Auto Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12332ccf",
   "metadata": {},
   "source": [
    "### Task:\n",
    "1. Build a GAN model and train the model with MNIST dataset.\n",
    "2. Update Discrimator network to maximize $\\log(D(x)) + \\log(1 - D(G(z)))$ and Generator network to minimize $\\log(1 - D(G(z))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5212efec",
   "metadata": {},
   "source": [
    "### Running notebooks on Google Colaboratory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d61cd",
   "metadata": {},
   "source": [
    "* Goto [Google Colab](https://colab.research.google.com/)  \n",
    "* Under `File` menu, click on `Upload notebook` and upload this notebook  \n",
    "* To run the notebook with GPU, click on `Runtime` menu, select `Change runtime type`, and select `GPU` as Hardware accelerator  \n",
    "  \n",
    "Note:  \n",
    "* If you see a `“RuntimeError: CUDA error: device-side assert triggered”` error, try running the notebook on CPU (by choosing `None` in Hardware accelerator as discussed above) to get a more detailed traceback for the error.  \n",
    "* If the notebook is running fine with CPU and the same error persists while using GPU, try restarting the runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf42bfb",
   "metadata": {},
   "source": [
    "Install the needed packages in the current Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc764f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "#\n",
    "# !{sys.executable} -m pip install numpy\n",
    "# !{sys.executable} -m pip install matplotlib\n",
    "# !{sys.executable} -m pip install torch\n",
    "# !{sys.executable} -m pip install torchvision\n",
    "# !{sys.executable} -m pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4324a067",
   "metadata": {},
   "source": [
    "PyTorch is an open source python package for building deep learning models. We will use this package in this exercise. For a basic understanding of the package, see [PyTorch introduction](https://pytorch.org/tutorials/beginner/basics/intro.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82b8f79",
   "metadata": {},
   "source": [
    "# Generative Adverserial Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f1cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75f70cf",
   "metadata": {},
   "source": [
    "### Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37eaee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "print(\"Random Seed: \", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e0fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499a53df",
   "metadata": {},
   "source": [
    "### Create the input data transformations and dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d01606",
   "metadata": {},
   "source": [
    "PyTorch has two primitives to work with data: `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`. `Dataset` stores the  data samples and their corresponding labels, and `DataLoader` wraps an iterable around the Dataset.  \n",
    "The `torchvision.datasets` module contains Dataset objects for many real-world vision data. We will use the MNIST dataset of handwritten digits in this exercise.  \n",
    "Every TorchVision Dataset includes two arguments: `transform` and `target_transform` to modify the data samples and labels respectively. You can use `torchvision.transforms.Compose` to compose multiple transformations together.  \n",
    "For our MNIST dataset, we can use the `torchvision.transforms.ToTensor` to convert the numpy array to a tensor, and `torchvision.transforms.Normalize` for normalizing the input to the range $\\left[-1, 1\\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the generated images (28x28).\n",
    "image_size = 28\n",
    "\n",
    "# Creating the transformations\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5), (0.5))]\n",
    ")\n",
    "\n",
    "# Define the train and test sets from MNIST data\n",
    "dataset = MNIST(\"./data\", transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9002d0",
   "metadata": {},
   "source": [
    "### Define the dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a88990",
   "metadata": {},
   "source": [
    "Here we define a batch size for the dataloader, i.e. each element in the dataloader iterable will return a batch of features and labels. Additionally, we can shuffle the data in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e7ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the batch size\n",
    "batch_size = 32\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba65f029",
   "metadata": {},
   "source": [
    "### Plot example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some training images\n",
    "images, labels = next(iter(dataloader))\n",
    "images = make_grid(\n",
    "    images[:batch_size],\n",
    "    padding=2,\n",
    "    normalize=True,\n",
    ").cpu()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"MNIST: Training Images\")\n",
    "plt.imshow(np.transpose(images, (1, 2, 0)))\n",
    "plt.show()\n",
    "\n",
    "print(\"Labels:\")\n",
    "print(labels.reshape(-1, 8).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb25ac",
   "metadata": {},
   "source": [
    "### Initialize the parameters of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ae98f",
   "metadata": {},
   "source": [
    "Define a function to initialize the paramters of the model as suggested in the [DCGAN paper](https://arxiv.org/pdf/1511.06434.pdf).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e922a3",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-97c028ba16d1113d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# mean = ...\n",
    "# std = ...\n",
    "\n",
    "mean = 0.0\n",
    "std = 0.02\n",
    "\n",
    "\n",
    "\n",
    "def weights_init(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        model.weight.data.normal_(mean, std)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(mean, std)\n",
    "        model.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7b84a8",
   "metadata": {},
   "source": [
    "### Define the generator class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701a33f6",
   "metadata": {},
   "source": [
    "Define a generator class with the following number of output feature maps in the hidden layers: $\\left[128, 64, 32\\right]$.\n",
    "\n",
    "Please refer [transposed convolution layer](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html),  [batch normalization layer](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html), and [activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) for defining the network.\n",
    "\n",
    "The class is to be inherited from the base [Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ee307",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f1b7a8412c8f7145",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the input latent dimension for Generator\n",
    "num_latent_features = 10\n",
    "\n",
    "# Defining the generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define the generator network to generate fake data.\n",
    "        # self.main = ...\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(num_latent_features, 128, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 1, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        output = self.main(z)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db7047",
   "metadata": {},
   "source": [
    "Instantiate the generator model and initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c479b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the generator\n",
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd30d64e",
   "metadata": {},
   "source": [
    "### Define the discriminator class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a06aa7",
   "metadata": {},
   "source": [
    "Define a discriminator class with the following number of output feature maps in the hidden layers:  \n",
    "$\\left[32, 64, 128\\right]$  \n",
    "\n",
    "Please refer [convolution layer](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), [activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity), and [batch normalization layer](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) for defining the network.  \n",
    "\n",
    "The class is to be inherited from the base [Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47152d8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c92be8407a4c0701",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Defining the discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define the discriminator network to output a scalar probability of real data.\n",
    "        # self.main = ...\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        output = self.main(data)\n",
    "        return output.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd266b08",
   "metadata": {},
   "source": [
    "Instantiate the generator model and initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca6def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the discriminator\n",
    "netD = Discriminator().to(device)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81f44ac",
   "metadata": {},
   "source": [
    "### Define the loss function and optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daedd71",
   "metadata": {},
   "source": [
    "Discriminator network is trained to maximize $\\log(D(x)) + \\log(1 - D(G(z)))$ and Generator network to minimize $\\log(1 - D(G(z))$.\n",
    "\n",
    "Binary cross entropy loss has both the terms we need to optimize for GAN.  \n",
    "\n",
    "$BCE_{n}=-\\left[y_{n} \\cdot \\log x_{n}+\\left(1-y_{n}\\right) \\cdot \\log \\left(1-x_{n}\\right)\\right]$  \n",
    "\n",
    "We need to choose the labels correctly while training to use the needed term from the loss function.  \n",
    "\n",
    "Also initialize optimizers for both discriminator and generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08c2463",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-656e3a493c72e918",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# optimizerD = ...\n",
    "# optimizerG = ...\n",
    "# criterion = ...\n",
    "\n",
    "learning_rate = 0.0002\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=learning_rate, betas=(beta_1, beta_2))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=learning_rate, betas=(beta_1, beta_2))\n",
    "criterion = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae85b1",
   "metadata": {},
   "source": [
    "### Define the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c623bd",
   "metadata": {},
   "source": [
    "We define the labels for real and fake samples.  \n",
    "Also a constant noise is defined which will help us in visualizing the training progress of generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch of latent vectors that we will use to visualize the progression of the generator\n",
    "fixed_noise = torch.randn(batch_size, num_latent_features, 1, 1, device=device)\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c4ade",
   "metadata": {},
   "source": [
    "A helper function to remove the normalization we added to the dataloader.  \n",
    "This is useful while plotting the images which require the values to be in the range $\\left[0, 1\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fb7456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(data, mean=0.5, std=0.5):\n",
    "    return mean + std * data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc54b08",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93caafb5",
   "metadata": {},
   "source": [
    "Train the discriminator to maximize $\\log(D(x)) + \\log(1 - D(G(z)))$ and Generator network to minimize $\\log(1 - D(G(z))$.   \n",
    "\n",
    "**Perform the following steps while training discriminator:**  \n",
    "* Iterate over batches of input data in each epoch.\n",
    "* PyTorch accumulates the parameter gradients on each subsequent backward pass, thus we need to set the gradients explicitly to zero in each iteration to avoid this. This can be done by calling function `zero_grad()` on the network.\n",
    "* Perform a forward pass in the discriminator with the input batch (ie., real images).\n",
    "* Define the labels for the input. Note that we only have two labels (ie., real and fake). `torch.full()` fills a tensor with a given value.\n",
    "* Calculate the loss for this batch of input (ie., maximize $\\log(D(x)$).\n",
    "* Calculate the gradients for the model parameters in the direction of minimizing the loss. The `backward()` function is used on the loss to calculate this gradients.\n",
    "* Perform a forward pass in the discriminator with the fake batch of images generated by the generator. `torch.randn()` can be used to generate random tensors. `detach()` function can be used on a tensor so that gradients are not backpropogated through this variable. This is needed as we dont need to calculate gradients for generator parameters while training discriminator.\n",
    "* Calculate the loss for this fake batch of input (ie., maximize $\\log(1 - D(G(z))$).\n",
    "* Calculate the gradients and perform the optimization of discriminator parameters using the gradients. The `step()` function is used on the optimizer to update the parameters.\n",
    "\n",
    "**Perform the following steps while training generator:**  \n",
    "* Perform a forward pass in the discriminator with the fake batch of images generated by the generator.\n",
    "* Calculate the generator's loss (ie., minimize $\\log(1 - D(G(z)))$), gradients for the generator parameters and optimize.\n",
    "\n",
    "⚠️ The training can take a long time, from minutes to half an hour or even more, depending on your hardware!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f9db7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4a2cfac5b68a2e1c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "\n",
    "# Lists to keep track of progress\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    with tqdm(dataloader, unit=\"batch\") as epoch_pbar:\n",
    "        for data in epoch_pbar:\n",
    "            epoch_pbar.set_description(f\"Epoch {epoch}\")\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            # train with real data\n",
    "            netD.zero_grad()\n",
    "            real_img, _ = data\n",
    "            real_img = real_img.to(device)\n",
    "\n",
    "            \"\"\"\n",
    "            Calculate the loss for the batch of real inputs and \n",
    "            calculate the gradients.\n",
    "            variable `errD_real` should hold the discriminator loss value for the batch of real inputs which \n",
    "            will be used to display the progress of training\n",
    "            \"\"\"\n",
    "            \n",
    "            label = torch.full(\n",
    "                (batch_size,), real_label, dtype=real_img.dtype, device=device\n",
    "            )\n",
    "            output = netD(real_img)\n",
    "            errD_real = criterion(output, label)\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            # train with fake data\n",
    "            \"\"\"\n",
    "            Generate fake data and calculate the loss for the batch of fake inputs and \n",
    "            calculate the gradients.\n",
    "            variable `errD_fake` should hold the discriminator loss value for the batch of fake inputs which \n",
    "            will be used to display the progress of training\n",
    "            \"\"\"\n",
    "            \n",
    "            noise = torch.randn(batch_size, num_latent_features, 1, 1, device=device)\n",
    "            fake_img = netG(noise)\n",
    "            label.fill_(fake_label)\n",
    "            output = netD(fake_img.detach())\n",
    "            errD_fake = criterion(output, label)\n",
    "            errD_fake.backward()\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "\n",
    "            \"\"\"\n",
    "            Calculate the loss for the batch of fake inputs and \n",
    "            optimize the generator parameters with the gradients.\n",
    "            variable `errG` should hold the generator loss value for the batch of fake inputs which \n",
    "            will be used to display the progress of training\n",
    "            \"\"\"\n",
    "            \n",
    "            label.fill_(real_label)\n",
    "            output = netD(fake_img)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            od = OrderedDict()\n",
    "            od[\"Dis_loss\"] = errD.item()\n",
    "            od[\"Gen_loss\"] = errG.item()\n",
    "            epoch_pbar.set_postfix(od)\n",
    "\n",
    "            # Save Losses for plotting later\n",
    "            G_losses.append(errG.item())\n",
    "            D_losses.append(errD.item())\n",
    "\n",
    "    # test and visualize\n",
    "    with torch.no_grad():\n",
    "        norm = plt.Normalize(0, 255)\n",
    "        fake_img = netG(fixed_noise)\n",
    "        fake_img = scale(fake_img.cpu())\n",
    "        fake_img = fake_img.view(batch_size, 1, 28, 28)\n",
    "        image_grid = make_grid(fake_img, nrow=8)\n",
    "        image_grid = image_grid.permute(1, 2, 0).numpy()\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Epoch {epoch}: fake images\")\n",
    "        plt.imshow(image_grid, cmap=\"gray\", norm=plt.Normalize(0, 255))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6c3ae9",
   "metadata": {},
   "source": [
    "### Plot the training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a2072",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses, label=\"Generator\")\n",
    "plt.plot(D_losses, label=\"Discriminator\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
